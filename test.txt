thank you so much for this invitation
Lor um I feel uh honored and embarrassed
to speak directly after Kristoff because
first of all how can I shine after this
but uh second also because I am going to
zoom out a little bit into uh Vis the
same Paradigm and talk a little bit more
about how the mechanisms that Kristoff
has been discussing will although could
lead to the phenomena and phenomenology
that we associate with Consciousness so
it's PR complimentary if you want to
indulge me I think when we want to build
a theory of Consciousness there are a
few things that we want it to be able to
deliver right so first of all it should
capture the correct phenomenology if our
model of Consciousness is not producing
and our understanding um what we are
pointing at when we talk about
Consciousness lexically right the
feeling of what it's like then this
theory is not going to be any good
because how would we know it's a theory
of
Consciousness it doesn't help if you
just rename Consciousness into the thing
that we currently have in our computer
and want uh to sell right we want to
explain how it's actually producing
phenomenal experience and maybe a
subject that reports on it and
self-reports on it then uh we want to
delineate this idea of Consciousness
against things that are not conscious
what are necessary and sufficient
conditions and then we want to explain
the causal structure that leads to the
phenomenology that is how is that that
I'm pointing at coming about what Cal
structure is going to lead to it and
then there some stretch goals it would
also be nice if you could explain how
this comes into being and why it's there
what does purpose does it serve and
maybe we can specify this process well
enough to spell out the theory in such
detail that we can implement it in a
computer and thereby test it whether it
actually produces the phenomenology that
we want to have for instance reporting
and self-reporting subject if you set it
in the right setting uh and if you
cannot do this maybe an Al alternative
would be that we specify an automatic
search process basically the search
space for process that would have that
uh ability and so rather than sitting
down and tinkering by hand until
something pops up that has the necessary
qualities maybe we can just leave this
to the machine like Al go and it's going
to um go through this surf space until
it finds a solution and we do some kind
automatic testing that lets us converge
there but this still means that we have
to specify the space in which we are
searching for
Solutions
so what is consciousness what's the
ontological status what kind of
phenomenon are we talking about and
first of all let's get some agreement
established I know that's going to be
difficult so bear with me it's one
possible perspective among many many and
this is the one I'm going to take in the
following first of all I think that uh
Consciousness is a process that
transforms representational type
patterns into others right which means
it's a function a function that is
transforming representations into other
representation
ations and it is virtual which means
it's not a physical phenomenon it's not
something that exists at the level of
atoms or Elementary particles or Quantum
phenomena or neurons all these things
are mechanical stuff as far as I'm
concerned and it might be important in
which way they work but they are
themselves not conscious there's
basically a bunch of cells and there's
activation patterns without between the
cells without lots of generality here
and it's only that stuff right and it
would be very useful for a bunch of
cells we can produce these activations
patterns among each other to know what
it would be like to be an organism that
navigates a social world what it would
be like to be a person that cares how it
would be like to have an attention that
reflects what it's singling out and so
on and so because they can themselves
not do this they create a similation of
this a
Similac and that's us right so it's an
as if thing Consciousness like the
personal self exists as if like all
software exists as if right in your
computer there's only transistors firing
and the software is a model that we
imposed on it to understand what they're
doing in some course grain way and this
structure is well defined enough narrow
enough to be a valid caal model but it
software only exists as if right and in
this way software is in its nature not a
thing it doesn't have an identity it's
law-like software is actually a physical
law right a word processor on your
computer says it's course grain level if
you arrange matter in the universe in
the following way and you look at it
from this perspective and this degree of
rtion following thing is going to happen
right this is the physical law so when
you write software you discover an
extremely specific physical law and I
what I suggest is that Consciousness is
in this category which means
Consciousness doesn't have an identity
so your Consciousness is not different
from mine it's also not the same as mine
because it's not in the category of
things that can be different or the same
it's an
operator right okay now that we got this
out out of the way let's get to the
phology um is consciousness really like
being a bad
my issue with this kind of approach is
that I don't really know what it's like
to be a bad I also don't know what it's
like to be a human or what it's like to
be an organism because I frankly I like
the Baseline I don't have anything to
compare this against so this is
something that does not really help me
it's it's something where I feel okay
this helps me to say this is the thing
that I point at when I see organisms
exclaiming oh my God I'm conscious
what's happening to me but uh beyond
that it doesn't really explain that much
and so there is more narrow
phenomenology that I want to look at so
for for instance I can see that a range
of conscious states that I'm in there's
something like a minimal Consciousness
that I might have in meditation or in
dreams or or just waking up in the
morning where it don't have a self model
yet and the world doesn't make a lot of
sense yet I have um attentional agency
in more expanded States and I have
constructive agency where I observe my
Consciousness actively messing with my
mental content and with my
interpretations of my percepts I might
have self awareness where I perceive
myself as an intentional or a personal
self that is driven by this
Consciousness and it's extending this
Consciousness uh and um I might also get
to the point where I am a person that
stands in front of an audience while
experiencing itself as conscious so this
is a range of conscious States and
they're all conscious I want to capture
them in such a theory and also want to
explain how I have to have that range
and at the core of that phenomenon I see
that there is a self-observing Observer
right there is some process that notices
that it's noticing while it's noticing
right this is this is some of the core
phenomena that is happening here so it's
generating representation of
observations and representations that
about the fact that you are observing
these representations right now and uh
what's really crucial about
Consciousness is it's not so much where
does it happen right it's when does it
happen and it always happens now right
it's is the main feature of
Consciousness it does not only just
always happen now it also is probably
the thing that creates now and this now
is a period in which I can fit my
perceptual content into a frame that is
completely coherent
where I there there's stuff in my
memories that basically drops out of
this coherence so I can no longer make
it congruent with what I perceive and
stuff in my expectation set is not yet
coherent with what I see but I have an
expanded sense of now subjectively
usually about um 3 seconds give or take
that is my present now that is being
created by Consciousness and so this
nness is something I need to explain
also this nness is not static it's
moving this present moment is always a
little bit Dynamic there are sometimes
states where it's not much happening and
this dynamicity is something close to an
identity function but it's a special
case so basically I can see State
transitions in my conscious perception
at any given moment so it's Dynamic and
what I see is that in Consciousness I
have access to the things that it
singles out so there contents features
objects that I'm attending to and then I
also am aware of the mode in which I'm
attending so usually I know whether what
I'm looking at is a percept that I
cannot change or an interpretation of a
pep that I can change or a memory that I
can retrieve but not really change or an
imagination that it can change every
which way right and this is a mode in
which I attend to the contents from the
perspective of Consciousness and it has
the self-reflexive mode so these are
phenomena that I want to explain and I
think that are somehow crucial and
definitive for the type of Consciousness
that I have so uh when we think about
where does Consciousness happen I think
Consciousness happens in the mind and
the way I understand the mind is that
it's something like a generative AI that
produce works very much like a game
right the world that I see around myself
is obviously not the physical universe
as far as I know the physical universe
is characterized by things like the
Shing our equation and quantum mechanics
and so on in it's such an arrangement
that it's very difficult for me to make
it intelligible right it's not the
physical universe that I'm seeing what
I'm seeing is colors sounds people and
so on it's all stuff that's very cor
grained and that could not physically
exist in the way in which I perceive it
right so what I perceive is very much
like what I perceive on the screen when
a computer runs a game engine that is
making extremely simplified physics so
the computer can deal with it in the
same way my brain is making extremely
simplified model of physics at a very
abstract level of course graining that
is uh compatible with me sampling the
world at less than 20 Hertz with my
sensory apparatus and uh trying to fit a
curve to it with at the spatial temporal
resolution that my brain can handle
right so it's a game engine in a sense
and additionally it has a no sphere
right so my mind has this B of the world
of stuff and space that I perceive as
you and me being together here and
everything in the world can be mapped in
there and then all the other stuff my
ideas and so on that I have that are
decoupled from this perceptual
representation of the world outside of
me and my proception and so on right and
so traditionally this is in philosophy
called rest extensa and res forgettin
and I think it's best to treat them both
as mental categories mental domains and
even more before this our ancestors
called this Earth and Heaven I believe
so Earth is the world model and Heaven
is the noosphere res
cogitans and of course then we have the
reflective self that is uh put like an
NPC in a game engine put a smack into
the middle of this uh simulated game
world it's me and I perceive emotions
that are generated outside of me and
myself is Downstream from them so I feel
myself being subject to veilance and to
perets and so this uh have a model of an
attentional self that is a reflexive
agent at some point and that can turn
itself into a personal so I experience
myself as a person in the world and so
basically I have an attentional agent
that acts as a conductor of my mental
orchestra that is interacting with the
motivation agent outside of me and the
perception agent and they basically are
parts of the cognitive architecture that
I'm in that is coupled to the
environment right it's like a conductor
in Orchestra Consciousness is not some
mental modular superpowers it's
basically like one of many instruments
in this orchestra that has a specific
task compared to the other instruments
environmental orchestra that uh compute
functions of the cognitive domains is
that it listens for incoherence and
tries to minimize this incoherence in
the orchestra so everybody is on the
same page and we get coherent behavior
that is coherent this motivational
constraints perceptual constraints and
so on and this is I think the purpose of
Consciousness to basically act as a
conductor that is increasing coherence
so how does this work and our own brain
as far as I can see it's a completely
self-organizing substrate so unlike a
GPU this is not organized by an engineer
who puts it onto a tiip and then imposes
an algorithm that is forced onto all the
transistors and have no choice but be to
behave according to the spec of a
designer and executes an algorithm that
somebody has written up and then is run
on that substr no matter whether it once
or not right and uh in our brain this is
very different because our brain is not
made out of uh aimless transistors it is
made out of single- cell animals that
have shared Destiny and need to survive
and they can only survive if they find
some kind of arrangement between each
other that allows the organism to find
food for the neurons
so this is the situation that they're in
and as a result they get some emerging
organization a self organization from
inside out and there's no point and no
point a global structure that is not the
result of some inside out growing self
organization right and so if you build
an information processing system like a
machine Learning System out of
completely self-organizing elements what
does this look like and I think what it
looks like is not anything like the
current machine learning algorithms it's
something where you need to create
increasing coherence so it's you get
create an island of coherence at first
something that has a coherent reward
language and semantic language of
thought so to speak and that is then
growing and is colonizing the cortex
into a coherent pattern that allows you
to explain reality and at the core of
this to make this happen this coherence
happen is the self-observing Observer
it's the minimal coherent pattern right
it's something that notices I'm here I'm
observing and so I'm growing out it's
quite beautiful because it's basically
the core of the card's meditation K agum
I think is not some philosophical
statement about ontology that allows you
to make a derivation it's the strategy
that you can introspectively observe
when you wake up in the morning right I
start out with oh here I am what is this
world around me how can I make sense of
it and then I Branch out and integrate
all these patterns until they become
percepts in World model right so uh you
need need to take these individual
agents the neurons and reward them by uh
giving them a quote that is um message
passing in it some of these messages
mean that say you did well in our
architecture or you didn't well and they
get rewarded for being trainable in this
way and uh the these individual units
they can exchange messages via
neurotransmitters or even complicated
functions potentially by exchanging RNA
with their neighbors and so on there's a
bunch of things that these cells can do
in principle and this determines the
search space for processes where
Consciousness is going to emerge right
it was has to emerge in the structure as
this process that is going to organize
it like a government emerges in a
society at some point and the government
works first of all by noticing that it
decides to be a government and what this
entails and then starts to impose
structure on on the environment around
it and it's competing with other Proto
governments until something takes over
right there's some kind of neural
Evolution so does Consciousness come
first or last a lot of people feel that
it's so Advanced that probably only
people are conscious and uh it's it's
really Pinnacle of uh development of
cognitive agency but I don't see people
becoming conscious after the PHD right
they really are conscious before they
can track a finger and that uh suggests
that maybe Consciousness is not the
result of you interacting with the world
very deeply as an organism but maybe
it's the prerequisite for learning
anything right it's maybe it's the
simplest learning algorithm that nature
can discover for a complex recurrent
brain-like structure that is
self-organizing that if that is the case
it's a hypothesis right uh then this
gives us a hint of how we could search
for it right we could set up something
that is cly self-organizing and is
incentivized to to process information
that is be rewarded if it succeeds in it
and punish if it doesn't and um then it
we might have a search space that is
small enough so the same thing happens
as it happens spontaneously in the brain
of every infant or a fetus and it's
organizing itself into a structure that
sudden you have this amazing pH
transitions where it starts to learn
efficiently more complicated things
right and so I suspect that
Consciousness might be discovered in our
brain before other learning algorithms
most of them they're probably simpler
heavier learnings like stuff but once
you go beyond this you probably need
constructive agency and Consciousness is
this basically an operator that the
brain is discovering to increase its
coherence globally and uh it is can
evolve and something like a Newar
something like an evolution that happens
in every single brain among
organizational patterns in the brain and
your genetics of course are going to
bias that Evolution so it's going to
converge much faster than otherwise
would but it seems still to be working
everybody right so no matter how mutated
you are no matter how many lesions you
get in your brain bra almost every time
it works you become conscious and then
you figure yourself out to some degree
right and if you don't you are vegetable
you never learn anything you never talk
to other people it's never going to work
out anything right so the Consciousness
is really the prerequisite but it also
is surprisingly resilient given the
number of things that you can do to an
infant or a fetal brain and it's still
going to develop into something viable
so it's basically something that
colonizes the brain with a structure
that is compatible with building a
learning architecture that is able to
interpret real
so Consciousness in the sense could be
actually the creator of our mind and our
world model and the self
model right so how could we generalize
the search space we start out uh with
using lots and lots of self organizing
reinforcement units that have for
instance a spatial neighborhood to each
other and then based on that spatial
neighborhood they pick a receptive field
so it's a selection function seen from
the perspective of each of these units
that basically choose where they read
information from in the environment this
determines the topology in which the
system is right and the second one is a
mapping function so you read out the
values from your receptive field and you
map them against your own internal state
to a new internal State some of which is
going to be exposed so other units can
read it right and this is the most
General way to look at it all the other
things like different neurotransmitters
are just types within that language that
emerg by setting one more or less bit in
it everything else can emerge from this
neur differentiation the different types
would just be changed in the internal
state so it's a very very general way of
looking at this in the most General case
then you can go down and narrow the
search space further down by imposing
further constraints but this seems to be
the thing that you get when you relax
the constraints maximally for having a
self reinforcement learning agents out
of self-organizing elements and our if
you take this Paradigm and combine it
with Neal networks neural networks are
in this class of systems existing neural
networks that use in AI but they make
very specific commitments that are
unlike the commitments that we have in a
biological brain so the selection
function is fixed for every neuron you
define the architecture before you start
training it you introduce more links
than you're going to need and you find
with setting many of those weights and
eventually to zero but you start out
with the potential space of links and
set this fixed and every neuron every
unit knows its receptive field the
beginning is not going to change the
second one the mapping function itself
is always a function over real numbers
that basically makes a weighted sum over
the values that it's reads from the
receptive fields and uh sour it against
the thresold like a ra you could also
use a Tang tangents or tangents
hyperbolical or some other functions but
people figured out that the original
perceptron idea just using a threshold
which means a relo today is good enough
and uh so this introduces a nonlinearity
which is a fancy way of saying if then
instead of just using weighted SS and uh
the mapping function also can change the
arrangement a bit and this happens only
during the training phase not during
inference not doing using the neural
network and uh you do this by changing
the weights which means basically Factor
s by which you multiply the individual
elements of the sums in in these
elements right and so if you look at
brains they are also in this class of
systems and here the selection function
is the space in which the urin are
located with respect to each other the
physical spatial neighborhood of the
neurons matters and then the learning uh
that they can perform on the neural
level by for instance changing synaptic
rates and CH changing things inside of
the neuron and change the receptive
field and there are some dritic growth
priors that tell you where the how the
neuron is going to change its receptive
field given the environment that it's in
and the mapping function itself is given
by learning and by evolved priors of the
individual neurons and it's somewhat
indeterministic so the neurons are not
deterministic as a result they're going
to population of neighboring neurons
that has the same receptive field is
going to sample a function space
stastically rather than executing only
one function right and so if we
generalize this we have a selection
function that takes learn Global and
local functions as for selection and
mapping function that can take arbitrary
automata and that can be a multi-
function BAS basically a nuring machine
and uh this allows us to think about
computation in brain like self
organizing distributed systems in very
very general way and so if Consciousness
is the simplest way to organize such
things it might be that it pops up right
and so the task of Consciousness in such
an architecture would be the definition
of a reward language credit assignment
to the individual units how much do do
they perform uh with respect to the
global reward uh and the evaluation that
uh what's happening in here in this
thing what is the largest possible now
that we can create inside of the system
the interpretation of features in terms
of how does this relate to our world
model how can we interpret all the
perceptual features and so on sensory
features patterns that show up in the
system as parts of a model of the world
and uh how can we perform construction
when we need to dis uate how we can we
learn how to reason and so on build lots
and lots of tools that allow us to
create a reality and fix it right does
AI have to be conscious uh but I think
that a GPU is not a self-organizing
system so obviously you can get a V
without doing this that's why present
systems like llms and Deli and so on are
so good they they work without being
conscious they can train themselves
without these self-organizing principles
and they do this quite well and people
say oh my God this is so expensive and
so on but imagine your task would be to
take 800 million pictures and correlate
them uh would you be able to within uh
the space of a couple weeks training
time uh
deduce a visual world in the same way as
Del or stable diffusion does no right
it's really amazing that this works at
scale and you can do this on a GPU farm
for mere $10 million at this point and
uh I would say that our brain is
probably using be given far too much
credit I guess but if you look at the
stable diffusion weights it's one lump
two gigabytes can download the weights
and this these two gigabytes of neural
network contain the entire visual
Universe all the Artistic Styles all the
dinosaurs all the spaceship all the
celebrities everything is in there you
can generate it all right and this is so
it's much more than every individual of
us could reproduce and this this like
80% of what our brain is doing in these
two gigabytes plus a lot more right than
what we can do so this should give us an
idea of how little you potentially might
need in terms of capacity to produce
what we are doing right so our brain of
course is usually redundant and so on uh
if you take out half the brain early
enough it's still going to work pretty
much the same way but it's uh it's
amazing that this works at all and also
these systems are not coupled to the
environment for us the world is
learnable because we are directly
coupled to it and we basically become
resonators with the world and buil a
resonant model that is the best that we
can do with this recurrent network with
the spatial temporal resolution and
modeling
depth should AI be conscious it's not
clear if it doesn't it has to right it
could be that the algorithm that I
Envision will never be very efficient on
Nvidia hardware and while they might
work work in principle turns out that
you need to train this system for 14
years until it's able to do anything and
for 26 years until it gets to a PhD
level right so this would be very
awkward but uh imagine that uh the
question should it be conscious imagine
we could do this should we be doing this
right there is this question seems to be
a little bit in bad taste to say I have
a product here and it's a conscious
agent and you can put put in your car or
a washing machine and whatever right but
who knows maybe we can build it in such
a way that it's okay with it and Society
adapt to this maybe there's nothing
wrong with this with having stuff that
is conscious everywhere whether It
suffers is really a question of the
setup same with people right and the uh
so there but there are applications that
are only enabled that are important with
conscious AI agents that are conscious
in a similar way as us and I think they
might be uh and one is for instance if
you think about perceptual empathy
perceptual empathy is a little bit
different than cognitive empathy I say
this especially for the Nerds among you
because some of you might not know but
but uh highly empathetic normal people
like not me uh they know that there is
perceptual empathy which means that you
perceive the emotions of other rather
than making inference over their facial
expressions in the context that are in
and when you do this you basically have
a bidirectional feedback loop to the
mind of the other person because you're
very close to them and you VIP with them
and it means that you can have mental
States together that you couldn't have
alone imagine that you could do this
with an AI imagine that you could have
an AI that's basically understanding
what you want to draw on the screen and
so intuitively that you feel it it's me
that is drawing this on the screen right
now and you this is mostly you do you do
make have a little bit manual input and
so and it's mostly observing you and
making real time inference on your
mental state because it's vibing with
you right and to do this you would need
to sample you at a high multiple of the
frequency at which your nervous system
operates and adapt to what's happening
in your nervous system by going into
resonance with it in a similar way as
people do this with each other right
that would be something that might be
enabled by this class of systems and it
might be super hard to do with a GPU
based machine learning algorithm that's
been trained offline another thing is
how could we align this AGI I'm
personally not a Doomer in any way but I
am an AI expectational list I expect
that a AGI is going to happen and
probably not very late in our future and
so we have to deal with it and it has to
deal with us and you can probably not
align something that is actually smarter
and more Lucid than you are because if
you are smart and do it nobody's going
to align you you align yourself right
how how would AI be different Al how can
you make sure that something that's able
to be smarter and more Lucid than you
and is going to align itself it wants to
coexist with you right it probably needs
to perceive that you are a conscious
being and value that and want to
integrate with your Som right it needs
to actually love you and so if you don't
if you are struck stuck with the system
for some reason because somebody at some
point will push the button and we will
have an AI that it might be smarter than
us uh it would be probably good if it
was conscious right so uh that is also a
point that I want to you guys to be in
mind is conscious ious all there could
be now I think Consciousness might
actually be quite boring there's
limitations like my perception now is
only 3 seconds come on this could be
much longer it's just my brain it's not
good enough to have longer perception
now also I only see one possible
interpretation at a time this is
ridiculous very often like I look at the
NEC Cube I know there's two
interpretations why do I only see one
this is a limitation of how my brain
operates right I want to see super
positions where they are and I want to
not have one perspective I want to have
all the possible perspectives that can
be generated in the decent mind right so
there are possible expect uh extensions
to Consciousness that are uh where
limitations of the Consciousness might
be limitations of my brain not of the
principles of the self-observing
Observer that is trying to interpret
reality in the best possible way and so
it might also be interesting to see how
we can build systems are actually
conscious in more interesting ways than
we are that's
